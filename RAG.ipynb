{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17NtC-wSuWV8QJFPD6P52n5qerT2eLqyk",
      "authorship_tag": "ABX9TyPbErvhffHWitvKYQsSDBj+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayank2177/Ask.ai/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Frameworks"
      ],
      "metadata": {
        "id": "0XD58fU9q2Pn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-qe_GqWIibsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0eb7f4-2d4c-496a-dd10-02d0acfb1565",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.3/254.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langchain_milvus langchain_google_genai pyPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install LangSmith"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3on-zEP7ktm",
        "outputId": "033cf04a-d5e8-4b8a-becb-075cbc0fb007",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: LangSmith in /usr/local/lib/python3.12/dist-packages (0.4.23)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from LangSmith) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from LangSmith) (3.11.3)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from LangSmith) (25.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from LangSmith) (2.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from LangSmith) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from LangSmith) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from LangSmith) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->LangSmith) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->LangSmith) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->LangSmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->LangSmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->LangSmith) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->LangSmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->LangSmith) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->LangSmith) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->LangSmith) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->LangSmith) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->LangSmith) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->LangSmith) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "uBUh2BzgqiWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d0b170-9857-4ca3-ea11-3c50b59ef485"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Gemini as Chat Model"
      ],
      "metadata": {
        "id": "6EZ4xJ3LrE_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "afEpXcB_qy3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e5f93a-da19-4184-90d8-62ebd43f7a40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Google Gemini: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using gemni as Embedding Model"
      ],
      "metadata": {
        "id": "dFgoEdowrMg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
      ],
      "metadata": {
        "id": "Yo-wFJ6OrYd_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Milvus as VectorStore"
      ],
      "metadata": {
        "id": "GeDouK3Src7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_milvus import Milvus\n",
        "import tempfile\n",
        "\n",
        "db_file = tempfile.NamedTemporaryFile(prefix=\"milvus_\", suffix=\".db\", delete=False).name\n",
        "print(f\"The vector database will be saved to {db_file}\")\n",
        "\n",
        "vector_db = Milvus(\n",
        "    embedding_function=embeddings_model,\n",
        "    connection_args={\"uri\": db_file},\n",
        "    auto_id=True,\n",
        "    index_params={\"index_type\": \"AUTOINDEX\"},\n",
        ")"
      ],
      "metadata": {
        "id": "IJp9FgCtror3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6476211-2fb9-46f7-83ea-11932fe0b167"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vector database will be saved to /tmp/milvus_6h5w4q_p.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load document from Drive"
      ],
      "metadata": {
        "id": "i51tt9gx3PTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-by7PUToFQt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the file path from the user\n",
        "file_path = input(\"Please enter the path to the text file in your Google Drive: \")\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.isfile(file_path):\n",
        "  print(f\"Error: File not found at {file_path}\")\n",
        "else:\n",
        "  filename = file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEDyNghFrxQf",
        "outputId": "a2e46b13-4f2b-402d-bf05-529bef6d98c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter the path to the text file in your Google Drive: /dl-curriculum.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download document from websource"
      ],
      "metadata": {
        "id": "fXnSVRHv6p0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Load contents from web page\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Au1W7qV26wHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the document into chunks"
      ],
      "metadata": {
        "id": "AQuoVqu33wfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader, UnstructuredEmailLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = []\n",
        "if not os.path.exists(filename):\n",
        "    raise FileNotFoundError(f\"File not found: {filename}\")\n",
        "elif filename.endswith(\".pdf\"):\n",
        "    loader = PyPDFLoader(filename)\n",
        "    documents.extend(loader.load())\n",
        "elif filename.endswith(\".docx\"):\n",
        "    loader = Docx2txtLoader(filename)\n",
        "    documents.extend(loader.load())\n",
        "elif filename.endswith(\".eml\"):\n",
        "    loader = UnstructuredEmailLoader(filename)\n",
        "    documents.extend(loader.load())\n",
        "elif filename.endswith(\".txt\"):\n",
        "    loader = TextLoader(filename)\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "doc_id = 0\n",
        "for text in texts:\n",
        "    text.metadata[\"doc_id\"] = (doc_id:=doc_id+1)\n",
        "\n",
        "print(f\"{len(texts)} text document chunks created\")"
      ],
      "metadata": {
        "id": "KBfGi5pLr6TQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf32aa79-d90d-4bab-fb78-a4bab0617e26"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22 text document chunks created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Populate the database"
      ],
      "metadata": {
        "id": "70hpF1ca3XFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = vector_db.add_documents(texts)\n",
        "print(f\"{len(ids)} documents added to the vector database\")"
      ],
      "metadata": {
        "id": "uZOyw7Ulw237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13c06be-f736-4775-bee1-7dcff321819e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22 documents added to the vector database\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conduct a similarity search"
      ],
      "metadata": {
        "id": "t8hgehse3au4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = str(input(\"Enter you Question: \"))\n",
        "docs = vector_db.similarity_search(query)\n",
        "print(f\"{len(docs)} documents returned\")\n",
        "      # Separator for clarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LAlKr165w68j",
        "outputId": "63716111-a673-41ef-8b35-4e10f6cccbaf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter you Question: What is Backpropagation\n",
            "4 documents returned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automate the RAG pipeline"
      ],
      "metadata": {
        "id": "W4zeXe1o4As5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "\n",
        "# Create a prompt for question-answering with the retrieved context\n",
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "# Assemble the retrieval-augmented generation chain\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm=model,\n",
        "    prompt=prompt_template,\n",
        ")\n",
        "\n",
        "rag_chain = create_retrieval_chain(\n",
        "    retriever=vector_db.as_retriever(),\n",
        "    combine_docs_chain=combine_docs_chain,\n",
        ")"
      ],
      "metadata": {
        "id": "4QO_kZjP3_yw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate a retrieval-augmented response to a question"
      ],
      "metadata": {
        "id": "zwacciZ9xt0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "crwQpd8AxmQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4597223f-1ad7-47d6-b33e-3256ffa029e5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backpropagation involves:\n",
            "*   Derivation using the chain rule.\n",
            "*   Computing gradients for each layer.\n",
            "*   Updating weights and biases.\n",
            "*   Understanding computational graphs.\n",
            "\n",
            "For Recurrent Neural Networks, there is also \"Backpropagation Through Time (BPTT)\", which involves unfolding the RNN, treating the RNN as a deep network over time, calculating gradients, and applying the chain rule through timesteps.\n"
          ]
        }
      ]
    }
  ]
}